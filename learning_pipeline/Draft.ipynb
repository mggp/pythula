{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9cb0a18-b203-4e77-8058-d7ddf6ae6fdc",
   "metadata": {},
   "source": [
    "# Description\n",
    "The idea is to build a model that helps with classifying python code snippets.\n",
    "\n",
    "Its output is 1 out of 4 classes:\n",
    "- Data Processing\n",
    "- Web/API Code\n",
    "- Algorithms/Logic\n",
    "- Machine Learning\n",
    "\n",
    "# Dataset\n",
    "Ideas:\n",
    "- Use sth like `CodeSearchNet`\n",
    "- Scrape github repos and leverage info from their tags\n",
    "- Create a synthetic dataset from tutorials or documentation\n",
    "\n",
    "Goal: Have 100 to 500 data points per category.\n",
    "\n",
    "# Preprocessing\n",
    "- Use a tokenizer like HuggingFace's AutoTokenizer\n",
    "\n",
    "# Training\n",
    "- Fine-tune something like CodeBERT\n",
    "- Loss: CrossEntropyLoss\n",
    "- Optimizer: Adam\n",
    "- Epochs: 5–10 (for initial demo-level)\n",
    "\n",
    "## Evaluation\n",
    "- Accuracy\n",
    "- Confusion matrix\n",
    "\n",
    "# Questions:\n",
    "- How to store/load model?\n",
    "- How to compare models?\n",
    "- Baseline vs. mine vs. huggingface/CodeBERTa-small-v1 vs. DistilBERT vs. ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a41cfb8a-2e95-4bb7-bbe6-099624e2e7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72931627-8ea3-4bc3-96be-1ace313e86f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5286beaf-bec5-47d9-ba69-8797b5fc4d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 9232,\n",
       " 19220,\n",
       " 1640,\n",
       " 102,\n",
       " 6,\n",
       " 428,\n",
       " 3256,\n",
       " 114,\n",
       " 10,\n",
       " 15698,\n",
       " 428,\n",
       " 35,\n",
       " 671,\n",
       " 10,\n",
       " 1493,\n",
       " 671,\n",
       " 741,\n",
       " 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_tokens = tokenizer.tokenize(\"def max(a,b): if a>b: return a else return b\")\n",
    "tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.eos_token]\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa42ca4b-d5a6-4e5a-9ccf-92d144a661a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1685,  0.3331,  0.0392,  ..., -0.2262, -0.3359,  0.3277],\n",
       "         [-1.0436,  0.3191,  0.3959,  ..., -0.4708, -0.1289,  0.5579],\n",
       "         [-0.9022,  0.5009,  0.1820,  ..., -0.4935, -0.5855,  0.6971],\n",
       "         ...,\n",
       "         [-0.4663,  0.2088,  0.5154,  ..., -0.1752, -0.3702,  0.5890],\n",
       "         [-0.4513,  0.4893,  0.4857,  ..., -0.3150, -0.6229,  0.3867],\n",
       "         [-0.1703,  0.3353,  0.0404,  ..., -0.2282, -0.3384,  0.3300]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 4.9750e-01, -3.6809e-01, -5.7454e-01,  1.0966e-01,  3.4282e-01,\n",
       "          7.0422e-02,  4.8956e-01, -3.5818e-01,  1.0147e-01, -3.2847e-01,\n",
       "          4.4498e-01,  2.5206e-02, -2.6024e-01,  1.1288e-01, -6.0998e-02,\n",
       "          5.9084e-01,  4.4048e-01, -5.2328e-01,  9.5334e-02,  3.2977e-01,\n",
       "         -3.4737e-01,  5.2274e-01,  3.5409e-01,  8.1222e-04, -4.9782e-02,\n",
       "          2.3763e-01,  1.2317e-01,  9.8236e-02,  4.9072e-01,  1.4917e-01,\n",
       "          1.0529e-01,  1.1462e-01,  2.6017e-01, -3.1969e-02, -3.4618e-01,\n",
       "         -5.9457e-02, -5.2262e-01,  1.7050e-01,  6.8206e-01, -3.1251e-01,\n",
       "         -3.8177e-01,  4.8588e-02, -1.5073e-02, -3.5364e-01,  9.7735e-02,\n",
       "          6.2407e-01,  1.7006e-01,  5.8128e-02, -2.4977e-01, -2.6303e-01,\n",
       "         -4.8945e-01,  4.2905e-01,  3.7614e-01,  1.8760e-01, -2.7240e-01,\n",
       "          1.2351e-01,  5.3280e-02, -1.4142e-01, -2.2490e-02, -3.2160e-01,\n",
       "         -4.2335e-01, -4.4173e-01,  6.4758e-02,  1.9320e-01, -1.0279e-01,\n",
       "         -1.9797e-01,  3.9336e-01,  1.7219e-02, -3.4886e-01,  4.3432e-02,\n",
       "         -3.0433e-01,  1.7277e-01,  2.3118e-03, -7.4777e-01, -1.1275e-01,\n",
       "          6.1676e-02, -6.1766e-01,  8.6231e-02,  6.2932e-01,  4.2631e-01,\n",
       "         -7.6890e-02,  3.5614e-01, -1.7805e-02,  2.9910e-01, -1.6042e-01,\n",
       "         -3.1510e-01,  5.1284e-01, -1.3771e-01,  1.5109e-01,  3.4099e-01,\n",
       "         -3.5993e-01, -6.8903e-01, -1.4666e-01,  1.5198e-01, -2.8632e-01,\n",
       "          1.9759e-01, -3.1253e-02,  1.1719e-01, -3.7307e-01, -6.5032e-02,\n",
       "          2.1109e-01, -2.4931e-01, -8.9592e-02,  2.4239e-01,  3.1544e-01,\n",
       "         -2.3738e-01, -2.3566e-01,  6.7964e-02,  1.1827e-01, -1.7191e-01,\n",
       "         -1.8114e-01,  6.3513e-01,  5.4469e-01,  1.8282e-01,  1.4556e-01,\n",
       "          1.0282e-01, -2.4123e-01, -3.6399e-01,  4.1590e-01, -4.0026e-01,\n",
       "          2.0248e-01, -1.6243e-01,  1.0541e-01,  3.1628e-01, -3.7827e-01,\n",
       "          2.0589e-01,  9.8366e-02,  4.7112e-01,  2.0192e-01, -1.0810e-01,\n",
       "         -2.5779e-02, -4.3047e-02, -6.2478e-02,  2.0629e-01, -1.1479e-02,\n",
       "          7.9146e-02,  1.6876e-01, -7.4072e-01, -3.9138e-01,  5.6004e-01,\n",
       "          7.8801e-01,  9.8383e-02,  2.1465e-01,  2.1456e-01,  4.9916e-01,\n",
       "          5.9873e-01,  2.7066e-01, -5.6034e-01,  7.7182e-04,  3.2314e-01,\n",
       "         -8.4422e-02, -9.8721e-02, -2.8894e-01, -5.1492e-01, -6.2231e-01,\n",
       "         -7.1195e-02,  3.7135e-01, -2.2594e-02,  8.9180e-03,  5.8837e-01,\n",
       "          2.3804e-01, -4.3125e-01, -2.1761e-01, -1.8206e-01, -1.9061e-01,\n",
       "         -4.0133e-01, -1.6558e-01, -3.6317e-02, -4.3803e-01, -4.5489e-01,\n",
       "         -1.2832e-02, -5.5757e-01, -3.4004e-02,  3.1681e-01, -5.2866e-01,\n",
       "          6.0848e-01, -5.4178e-01,  1.3073e-01,  5.0709e-01, -4.0953e-01,\n",
       "          1.1111e-01, -4.9435e-01,  6.8678e-02,  4.0986e-01,  9.3627e-02,\n",
       "          3.1640e-01, -2.8635e-01,  4.7105e-01, -4.5707e-02,  2.1905e-01,\n",
       "          2.6631e-01,  1.4883e-02, -3.2077e-01,  3.5038e-01, -4.5549e-01,\n",
       "          2.2594e-01, -3.8315e-01, -1.2406e-01, -3.8408e-01, -3.3107e-01,\n",
       "          1.9691e-01, -7.9483e-01, -5.7436e-01,  6.0311e-02, -1.3771e-01,\n",
       "          3.1008e-02, -7.0998e-02,  3.6987e-02,  5.6002e-02, -1.3018e-01,\n",
       "          2.8744e-02, -4.5530e-01,  3.7241e-01, -2.9324e-01, -2.2123e-01,\n",
       "         -6.2677e-02,  3.1586e-01,  4.1470e-01,  2.2050e-01, -5.3521e-01,\n",
       "         -3.7554e-01,  1.9667e-01,  4.7342e-01, -1.6239e-01,  5.9071e-01,\n",
       "         -2.0297e-01, -2.5248e-01,  4.9344e-03,  3.1409e-01,  1.5819e-01,\n",
       "          5.3888e-01, -3.3149e-01, -4.4422e-02,  6.7460e-02, -5.3586e-01,\n",
       "         -3.1176e-01, -2.4766e-01,  2.4565e-01,  4.4887e-01,  4.8352e-02,\n",
       "          1.8375e-01,  4.1069e-01,  3.0484e-01,  9.3303e-03,  4.3717e-01,\n",
       "         -1.8158e-01,  2.3197e-01, -4.8631e-01, -4.8303e-02, -4.4866e-01,\n",
       "         -3.8262e-01, -4.7719e-01,  7.2491e-01, -3.2524e-01,  4.9916e-01,\n",
       "          5.2499e-01, -3.6963e-01, -2.3948e-01,  2.6310e-01,  1.5191e-01,\n",
       "          1.6487e-01, -2.7358e-02,  4.3545e-02,  2.1092e-01,  8.1634e-02,\n",
       "          3.5018e-01,  5.5283e-01,  2.3565e-01,  3.7428e-01, -4.7256e-03,\n",
       "          4.6579e-02,  4.0812e-01, -1.7120e-01, -1.8692e-01, -4.0550e-02,\n",
       "          1.7061e-01, -2.3378e-01, -4.1320e-01,  2.6783e-04, -3.6651e-02,\n",
       "          6.2209e-01, -1.1946e-01, -3.6498e-01,  2.2925e-01,  2.6719e-01,\n",
       "         -6.2986e-01,  1.2402e-01,  1.7832e-01,  2.0149e-01, -5.0035e-01,\n",
       "         -1.0863e-01,  3.3841e-02,  2.1838e-01, -5.4170e-01, -4.6367e-01,\n",
       "          5.6931e-01, -9.0834e-04,  2.9940e-01,  2.3746e-01, -3.1228e-01,\n",
       "         -2.8088e-01,  7.6634e-01, -1.2276e-01, -4.7541e-01,  3.5624e-01,\n",
       "          1.6490e-01,  4.0736e-03,  2.5346e-01,  1.7811e-01,  3.6203e-01,\n",
       "         -2.7857e-01,  5.0861e-01,  2.2278e-01, -6.1800e-01, -4.1455e-01,\n",
       "         -2.4309e-01, -9.8205e-04, -1.2631e-02, -3.2109e-01, -5.2967e-01,\n",
       "          8.6065e-02,  2.2637e-01, -2.0267e-01,  3.2809e-01,  2.1909e-01,\n",
       "          3.5374e-01, -5.1346e-02,  4.7302e-01, -2.8101e-01,  5.3436e-01,\n",
       "         -5.8113e-02,  5.5476e-01, -5.1958e-01, -8.1967e-02, -7.1035e-02,\n",
       "         -1.8847e-01, -4.9061e-02,  2.7026e-02,  3.5189e-01, -2.7104e-01,\n",
       "         -5.6880e-01,  2.3940e-01,  2.5550e-01,  1.7921e-01,  2.8563e-01,\n",
       "          4.4189e-01,  1.6059e-01,  1.6586e-01,  3.7000e-02,  2.8754e-01,\n",
       "          2.7540e-01, -2.4854e-01, -6.8635e-01,  2.9518e-01, -4.5267e-01,\n",
       "         -6.4859e-02, -2.4385e-01, -2.9751e-01,  6.9105e-01, -2.8178e-01,\n",
       "          2.4190e-01,  3.9338e-01,  2.7151e-01,  2.4004e-01,  5.1353e-02,\n",
       "          2.7501e-01,  2.7901e-01,  1.4271e-01,  9.9427e-02, -1.6383e-02,\n",
       "         -3.7496e-01, -3.1621e-01, -2.0294e-01,  2.2598e-01, -3.4380e-01,\n",
       "         -5.2909e-01,  1.1344e-01,  5.0209e-01,  1.1508e-01, -3.4032e-01,\n",
       "          3.4861e-01,  3.5542e-01,  1.7158e-01,  1.0434e-02, -2.2489e-01,\n",
       "         -5.4942e-02,  6.1403e-01, -1.0975e-01,  1.7118e-01,  7.3632e-01,\n",
       "          2.5192e-01, -5.3250e-01, -1.2624e-01, -2.7634e-01,  1.3144e-01,\n",
       "          1.8390e-01, -3.1698e-01,  2.5134e-01,  4.8554e-01,  3.0017e-01,\n",
       "          7.3450e-01,  5.8561e-02,  1.0687e-01,  1.0761e-01,  2.4819e-01,\n",
       "         -1.0415e-01,  1.1396e-01,  6.4847e-02,  5.1507e-01,  4.5176e-01,\n",
       "         -3.9162e-01,  5.6699e-02, -2.1709e-01, -8.5214e-02, -1.7616e-01,\n",
       "         -4.7523e-01, -1.2134e-02,  3.3794e-01, -5.7909e-01,  6.4076e-02,\n",
       "         -2.1752e-01,  1.1341e-01, -8.7520e-02, -1.0765e-01, -2.3029e-01,\n",
       "         -4.0820e-01,  6.8080e-01, -3.9544e-02, -1.0757e-01, -4.4884e-01,\n",
       "         -4.0001e-01,  6.1110e-02,  2.0842e-01, -3.3845e-01, -2.4540e-01,\n",
       "          4.8759e-01, -6.4019e-02, -5.7059e-02, -1.6874e-01,  2.4369e-01,\n",
       "         -2.4259e-01,  2.5890e-01,  2.6719e-01, -2.1164e-01, -2.5709e-01,\n",
       "          7.0956e-02, -4.0929e-01, -3.3671e-01, -4.9956e-01,  5.5042e-01,\n",
       "         -3.4912e-01, -2.0008e-01, -2.3730e-01, -6.1538e-01,  1.3825e-01,\n",
       "          3.2920e-01,  5.2705e-01, -3.1624e-01,  1.0704e-01,  7.8889e-01,\n",
       "         -9.5615e-02, -3.5749e-01,  1.1383e-02,  3.4800e-01, -7.4367e-02,\n",
       "          6.4183e-01,  5.2562e-01, -2.2438e-02,  1.6308e-01,  7.2264e-01,\n",
       "          5.2338e-02,  1.9646e-01, -2.9163e-01,  5.3137e-01, -8.5046e-02,\n",
       "          2.9320e-01,  2.5419e-02,  5.0183e-02, -4.6959e-03, -1.2442e-01,\n",
       "          3.9640e-01,  4.8759e-01, -6.8770e-01, -2.0909e-01,  1.7391e-01,\n",
       "          4.1043e-02, -2.0719e-01, -4.2706e-01, -1.7395e-01, -3.4407e-01,\n",
       "         -1.2792e-01, -1.5234e-01, -7.6935e-02, -4.8482e-01, -1.4220e-01,\n",
       "          3.1179e-01, -4.3819e-01,  1.5216e-01,  6.0492e-01, -2.0609e-02,\n",
       "          3.0679e-01, -4.0934e-01, -8.8663e-02,  3.6965e-01, -1.8421e-01,\n",
       "         -3.6191e-01,  1.8609e-01,  8.4222e-01, -2.4798e-01, -7.3185e-01,\n",
       "         -6.7578e-02,  3.7685e-01,  6.6734e-02,  2.0544e-01, -1.0685e-01,\n",
       "         -3.0224e-01,  8.9721e-02, -9.9897e-04,  8.0792e-02, -2.6428e-01,\n",
       "         -5.8369e-01, -2.6540e-01,  5.9132e-01, -6.0704e-01,  8.4323e-02,\n",
       "         -1.7520e-01, -1.2734e-02, -4.6975e-01,  2.3337e-01, -3.4770e-01,\n",
       "          6.6131e-01,  1.6826e-01, -5.7643e-01,  1.0542e-01,  6.1783e-02,\n",
       "         -7.7056e-02, -1.7524e-01, -2.0857e-01,  7.0767e-01, -3.6453e-01,\n",
       "         -8.0199e-01,  1.8301e-01,  2.3899e-01,  6.3045e-01, -2.8039e-01,\n",
       "         -5.5762e-02, -2.3400e-01,  2.3956e-01,  1.8500e-01,  6.3159e-03,\n",
       "         -2.2380e-01, -1.7693e-01, -5.6956e-01, -3.7252e-01, -5.2656e-01,\n",
       "          1.0625e-01,  1.3280e-01, -5.8075e-01,  2.6341e-01, -1.4378e-01,\n",
       "          3.2977e-01,  2.0335e-01, -3.7786e-01, -1.0401e-01,  4.0183e-01,\n",
       "          4.7947e-01,  2.4861e-01,  4.9808e-01,  2.7402e-01,  2.3218e-01,\n",
       "         -2.4913e-01,  1.5188e-01,  1.7746e-01, -1.2191e-01,  4.9082e-01,\n",
       "         -1.4955e-01, -6.2619e-01, -1.0400e-01,  7.2152e-01,  9.9393e-02,\n",
       "         -3.5058e-01, -7.9178e-03,  5.9675e-01,  2.0104e-01,  7.0921e-02,\n",
       "          3.0117e-01, -4.0452e-01, -2.3563e-01, -7.2239e-02, -7.9153e-02,\n",
       "         -4.4002e-01, -2.4151e-01, -7.3683e-02, -1.5744e-01, -2.9276e-01,\n",
       "         -9.0456e-02, -2.2447e-01,  3.8692e-01, -5.8112e-01, -8.5422e-02,\n",
       "         -1.2147e-01, -9.2773e-02, -2.6203e-01,  1.5819e-01,  1.9313e-01,\n",
       "          3.7981e-02,  9.2126e-02,  6.3672e-01, -1.1875e-01, -1.6367e-01,\n",
       "         -2.0311e-01, -2.3932e-01,  2.7866e-01, -3.2459e-01,  9.4900e-02,\n",
       "         -1.0008e-01,  3.1609e-01, -5.9932e-01, -2.7006e-01,  7.5978e-04,\n",
       "         -2.6134e-01, -4.1266e-01,  4.4862e-01,  2.6931e-01,  2.1707e-01,\n",
       "          5.3670e-01,  9.6922e-02,  5.2163e-02, -3.3105e-01, -3.8879e-01,\n",
       "         -2.5432e-01,  2.3009e-01, -7.5707e-03, -4.7482e-01, -2.0281e-01,\n",
       "          2.3660e-01, -4.9760e-01, -2.5120e-01,  3.5086e-01, -9.4184e-04,\n",
       "         -1.8856e-01, -2.3253e-01, -1.5125e-01, -6.6113e-01,  1.6864e-01,\n",
       "          1.0934e-01,  7.1617e-02, -2.2990e-01,  1.0219e-01, -2.2715e-01,\n",
       "          1.8264e-01,  2.5516e-01,  9.1116e-02, -1.7003e-01, -4.3305e-01,\n",
       "         -4.9145e-01, -3.2139e-01,  5.7903e-02,  3.6657e-01, -2.2949e-01,\n",
       "         -2.8063e-01,  1.1617e-01,  3.6120e-01, -1.7741e-01,  1.1957e-01,\n",
       "          1.7351e-01, -6.6759e-01, -2.1306e-01, -4.7903e-02,  1.0976e-01,\n",
       "         -1.6574e-02, -2.5477e-01, -4.2668e-01,  3.1086e-01, -7.8112e-02,\n",
       "         -2.5410e-01,  5.9363e-01, -2.3090e-01,  4.0536e-01, -3.1344e-02,\n",
       "         -3.6027e-01, -1.7356e-01,  4.4256e-01, -2.4134e-02,  3.2344e-01,\n",
       "          1.2147e-01, -5.6970e-01, -6.8544e-02, -3.5827e-02, -2.9050e-01,\n",
       "         -2.6791e-01, -1.2770e-01, -1.2066e-01,  3.3045e-01, -6.2850e-01,\n",
       "          4.3790e-01,  8.3780e-02,  2.0555e-01,  2.2921e-01, -4.3751e-01,\n",
       "         -2.8730e-01,  2.6541e-01,  3.6385e-01, -2.7253e-01, -5.5142e-01,\n",
       "         -4.3237e-01, -3.5846e-01, -3.5458e-01, -2.6223e-01,  5.9106e-01,\n",
       "         -9.3076e-02, -2.7750e-01, -2.3211e-01,  5.5739e-01,  1.2421e-01,\n",
       "          1.8462e-03,  4.1556e-01,  1.6861e-01,  1.6488e-01,  2.2025e-01,\n",
       "         -7.3772e-01,  2.5997e-01, -3.9701e-01,  4.5173e-03, -8.4839e-02,\n",
       "          2.6684e-01, -3.0623e-01,  3.8680e-02, -3.4479e-01,  1.2279e-01,\n",
       "          3.9140e-01, -4.0717e-01, -1.4384e-01,  3.5290e-01,  2.1646e-01,\n",
       "         -3.4454e-01,  5.9429e-02,  1.9127e-01,  3.4930e-01,  1.3720e-01,\n",
       "          1.0680e-01,  4.4797e-01, -4.7895e-01,  3.2598e-03, -4.5932e-01,\n",
       "         -5.6631e-01,  2.2528e-01,  1.3527e-01,  3.9478e-01, -1.3899e-01,\n",
       "         -3.7747e-01,  6.4856e-01,  5.4442e-02, -7.5439e-02,  2.8162e-01,\n",
       "         -8.9302e-03, -5.4320e-02, -2.5094e-01,  1.9559e-01,  6.2133e-01,\n",
       "         -6.4973e-02,  9.0919e-02,  4.4535e-01, -5.6958e-01,  3.5233e-01,\n",
       "         -1.3435e-01, -1.8565e-02,  8.9066e-02]], device='cuda:0',\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(token_ids).to(device)[None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10318794-379b-437d-9627-f8cc875dc16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "sample_code = \"def max(a,b): if a>b: return a else return b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aa8cb9e-f189-4c10-a2fd-061a5b91e481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 9232, 19220, 1640, 102, 6, 428, 3256, 114, 10, 15698, 428, 35, 671, 10, 1493, 671, 741, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sample_code, padding=\"max_length\", truncation=True, max_length=256# # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e786d42-887f-475c-ad55-2cb9935b9a89",
   "metadata": {},
   "source": [
    "# Fine-tuning v0\n",
    "## Create dataset\n",
    "This also creates the label2id and id2label maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4c2125d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"Data Processing\", \"Web/API Code\", \"Algorithms/Logic\", \"Machine Learning\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9a30fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"data/balanced_data.jsonl\", split=\"train\")\n",
    "dataset = dataset.map(lambda x: {\"label\": label2id[x[\"label\"]]})\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc81e3-5b54-4091-a659-7300370edd6b",
   "metadata": {},
   "source": [
    "## Create tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a4e46def-4ace-4e7f-994c-2c71d441ab2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea008a023a154f388a2fa0cb07e2a46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c33fcba65364e23a57882edc9786a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"code\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d1cbe06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['code', 'label', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 36\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['code', 'label', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 9\n",
       "     })\n",
       " }),\n",
       " RobertaTokenizerFast(name_or_path='microsoft/codebert-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       " \t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       " }\n",
       " ))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c62cbeb-84b0-4e19-bad2-96babc09a37c",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b98fb62c-22bf-43e0-ad38-e65e378f8230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/codebert-base\",\n",
    "    num_labels=4,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e2eff3-123a-4f1a-b7a5-25d1bd89be67",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1030af7c-11e6-4494-b84c-48a504152d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./codebert-finetuned\",\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_on_start=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3bdf5543-4cb7-4a1b-afb4-4f04dd79cf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_580285/1908840112.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5fcfea27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1917643547058105,\n",
       " 'eval_model_preparation_time': 0.0036,\n",
       " 'eval_runtime': 0.1914,\n",
       " 'eval_samples_per_second': 47.03,\n",
       " 'eval_steps_per_second': 5.226}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c34ec-8998-4b47-babd-6e2ec3c3a977",
   "metadata": {},
   "source": [
    "## Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7dff51ae-4ceb-4377-9d4a-dd697d855d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_code(code_snippet):\n",
    "    inputs = tokenizer(code_snippet, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    outputs = model(**inputs.to(device))\n",
    "    probs = outputs.logits.softmax(dim=1)\n",
    "    pred = probs.argmax(dim=1).item()\n",
    "    return id2label[pred], probs[0][pred].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8d55569-b7e5-4bf0-8e7d-853de701964f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Data Processing', 0.335357129573822)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_code = \"\"\"\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\"\"\"\n",
    "classify_code(sample_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf35a5d-f5cc-4c61-a1ed-cec65aa40682",
   "metadata": {},
   "source": [
    "Estaría bueno que el scraper acumule en lugar de borrar. Que se guarde el hash para no duplicar cosas y que haya un sistema de reviews. Pienso que estaría genial que muestre el código formateado, que haya un botón para aprobar y pasar al siguiente o reprobar y hacer un soft delete antes de avanazar. Tiene que ser un soft delete para que no se vuelvan a agregar y revisar códigos ya revisados. \n",
    "\n",
    "Quizá incluso estaría bueno que no se procese dos veces la misma página. Que no se guarde sólo un pedazo de código (o su hash), sino una URI. Y que no se tomen dos pedazos de código del mismo lugar. Así, la diversidad será máxima.\n",
    "\n",
    "Pero todo esto puede quedar para una segunda revisión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6fb34f",
   "metadata": {},
   "source": [
    "# Limpiar dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61582f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3 as sqlite\n",
    "import json\n",
    "\n",
    "db_path = \"schemas/code_snippets.db\"\n",
    "random_seed = 42\n",
    "\n",
    "conn = sqlite.connect(db_path)\n",
    "df = pd.read_sql(\"SELECT * FROM snippets ORDER BY hash\", conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc06f2",
   "metadata": {},
   "source": [
    "Es probable que esto quede desbalanceado. Hay que asegurarse que no sea el caso en el dataset de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "99829eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Se tomarán 8 ejemplos por clase para balancear.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "Web/API Code        13\n",
       "Data Processing     13\n",
       "Algorithms/Logic     9\n",
       "Machine Learning     8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = df[\"label\"].value_counts()\n",
    "min_count = counts.min()\n",
    "\n",
    "print(f\"\\nSe tomarán {min_count} ejemplos por clase para balancear.\")\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52403d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "balanced = (\n",
    "    df.groupby(\"label\", group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=min_count, random_state=random_seed))\n",
    "    .reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e848a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_file = \"data/balanced_data.jsonl\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in balanced.iterrows():\n",
    "        record = {\n",
    "            \"code\": row[\"code\"],\n",
    "            \"label\": row[\"label\"]\n",
    "        }\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d86138",
   "metadata": {},
   "source": [
    "# Fine-tuning v1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecfc3c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db0896893314522931cb5160d52d4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_704755/2798438065.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.382223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.436200</td>\n",
       "      <td>4.344590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.140700</td>\n",
       "      <td>3.220225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=6.032981872558594, metrics={'train_runtime': 9.6268, 'train_samples_per_second': 7.895, 'train_steps_per_second': 1.039, 'total_flos': 9998399643648.0, 'train_loss': 6.032981872558594, 'epoch': 2.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Domain-specific labels\n",
    "label_list = [\"Data Processing\", \"Web/API Code\", \"Algorithms/Logic\", \"Machine Learning\"]\n",
    "label_count = len(label_list)\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Training utilities\n",
    "def load_data():\n",
    "    dataset = load_dataset(\"json\", data_files=\"data/balanced_data.jsonl\", split=\"train\")\n",
    "    dataset = dataset.map(lambda x: {\"label\": label2id[x[\"label\"]]})\n",
    "    dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"code\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "# Initialize model, tokenizer, and dataset\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "output_dir = \"./codebert-finetuned\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=label_count,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dataset = load_data()\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Init trainer\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=0.02,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_on_start=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91081e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39eb85c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Algorithms/Logic', 0.5787521004676819)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify_code(code_snippet):\n",
    "    inputs = tokenizer(code_snippet, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    outputs = model(**inputs.to(device))\n",
    "    probs = outputs.logits.softmax(dim=1)\n",
    "    pred = probs.argmax(dim=1).item()\n",
    "    return id2label[pred], probs[0][pred].item()\n",
    "\n",
    "sample_code = \"\"\"\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\"\"\"\n",
    "classify_code(sample_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "924f162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dest = \"./codebert-finetuned/final_model\"\n",
    "model.save_pretrained(model_dest, max_shard_size=\"5GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "95df9cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./codebert-finetuned/final_model and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "new_model = AutoModel.from_pretrained(model_dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044abba6",
   "metadata": {},
   "source": [
    "## Optimize with LoRA (doesn't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d053adc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "! pip install -q peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a5d44855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa35e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, \n",
    "    r=2, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1,\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config, adapter_name=\"lora_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "69e87515",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = get_peft_model(new_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50267732",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RobertaForSequenceClassification' object has no attribute 'print_trainable_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_trainable_parameters\u001b[49m()\n",
      "File \u001b[0;32m~/localwork/pythula/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1962\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1960\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1961\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1962\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1963\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1964\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RobertaForSequenceClassification' object has no attribute 'print_trainable_parameters'"
     ]
    }
   ],
   "source": [
    "new_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d6c771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    num_train_epochs=50,\n",
    "    learning_rate=0.02,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_on_start=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, # The get_peft model doesn't work...\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32cb0c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['roberta.encoder.layer.0.attention.self.query.base_layer.weight', 'roberta.encoder.layer.0.attention.self.query.base_layer.bias', 'roberta.encoder.layer.0.attention.self.query.lora_A.lora_1.weight', 'roberta.encoder.layer.0.attention.self.query.lora_B.lora_1.weight', 'roberta.encoder.layer.0.attention.self.value.base_layer.weight', 'roberta.encoder.layer.0.attention.self.value.base_layer.bias', 'roberta.encoder.layer.0.attention.self.value.lora_A.lora_1.weight', 'roberta.encoder.layer.0.attention.self.value.lora_B.lora_1.weight', 'roberta.encoder.layer.1.attention.self.query.base_layer.weight', 'roberta.encoder.layer.1.attention.self.query.base_layer.bias', 'roberta.encoder.layer.1.attention.self.query.lora_A.lora_1.weight', 'roberta.encoder.layer.1.attention.self.query.lora_B.lora_1.weight', 'roberta.encoder.layer.1.attention.self.value.base_layer.weight', 'roberta.encoder.layer.1.attention.self.value.base_layer.bias', 'roberta.encoder.layer.1.attention.self.value.lora_A.lora_1.weight', 'roberta.encoder.layer.1.attention.self.value.lora_B.lora_1.weight', 'roberta.encoder.layer.2.attention.self.query.base_layer.weight', 'roberta.encoder.layer.2.attention.self.query.base_layer.bias', 'roberta.encoder.layer.2.attention.self.query.lora_A.lora_1.weight', 'roberta.encoder.layer.2.attention.self.query.lora_B.lora_1.weight', 'roberta.encoder.layer.2.attention.self.value.base_layer.weight', 'roberta.encoder.layer.2.attention.self.value.base_layer.bias', 'roberta.encoder.layer.2.attention.self.value.lora_A.lora_1.weight', 'roberta.encoder.layer.2.attention.self.value.lora_B.lora_1.weight', 'roberta.encoder.layer.3.attention.self.query.base_layer.weight', 'roberta.encoder.layer.3.attention.self.query.base_layer.bias', 'roberta.encoder.layer.3.attention.self.query.lora_A.lora_1.weight', 'roberta.encoder.layer.3.attention.self.query.lora_B.lora_1.weight', 'roberta.encoder.layer.3.attention.self.value.base_layer.weight', 'roberta.encoder.layer.3.attention.self.value.base_layer.bias', 'roberta.encoder.layer.3.attention.self.value.lora_A.lora_1.weight', 'roberta.encoder.layer.3.attention.self.value.lora_B.lora_1.weight', 'roberta.encoder.layer.4.attention.self.query.base_layer.weight', 'roberta.encoder.layer.4.attention.self.query.base_layer.bias', 'roberta.encoder.layer.4.attention.self.query.lora_A.lora_1.weight', 'roberta.encoder.layer.4.attention.self.query.lora_B.lora_1.weight', 'roberta.encoder.layer.4.attention.self.value.base_layer.weight', 'roberta.encoder.layer.4.attention.self.value.base_layer.bias', 'roberta.encoder.layer.4.attention.self.value.lora_A.lora_1.weight', 'roberta.encoder.layer.4.attention.self.value.lora_B.lora_1.weight', 'roberta.encoder.layer.5.attention.self.query.base_layer.weight', 'roberta.encoder.layer.5.attention.self.query.base_layer.bias', 'roberta.encoder.layer.5.attention.self.query.lora_A.lora_1.weight', 'roberta.encoder.layer.5.attention.self.query.lora_B.lora_1.weight', 'roberta.encoder.layer.5.attention.self.value.base_layer.weight', 'roberta.encoder.layer.5.attention.self.value.base_layer.bias', 'roberta.encoder.layer.5.attention.self.value.lora_A.lora_1.weight', 'roberta.encoder.layer.5.attention.self.value.lora_B.lora_1.weight', 'roberta.encoder.layer.6.attention.self.query.base_layer.weight', 'roberta.encoder.layer.6.attention.self.query.base_layer.bias', 'roberta.encoder.layer.6.attention.self.query.lora_A.lora_1.weight', 'roberta.encoder.layer.6.attention.self.query.lora_B.lora_1.weight', 'roberta.encoder.layer.6.attention.self.value.base_layer.weight', 'roberta.encoder.layer.6.attention.self.value.base_layer.bias', 'roberta.encoder.layer.6.attention.self.value.lora_A.lora_1.weight', 'roberta.encoder.layer.6.attention.self.value.lora_B.lora_1.weight', 'roberta.encoder.layer.7.attention.self.query.base_layer.weight', 'roberta.encoder.layer.7.attention.self.query.base_layer.bias', 'roberta.encoder.layer.7.attention.self.query.lora_A.lora_1.weight', 'roberta.encoder.layer.7.attention.self.query.lora_B.lora_1.weight', 'roberta.encoder.layer.7.attention.self.value.base_layer.weight', 'roberta.encoder.layer.7.attention.self.value.base_layer.bias', 'roberta.encoder.layer.7.attention.self.value.lora_A.lora_1.weight', 'roberta.encoder.layer.7.attention.self.value.lora_B.lora_1.weight', 'roberta.encoder.layer.8.attention.self.query.base_layer.weight', 'roberta.encoder.layer.8.attention.self.query.base_layer.bias', 'roberta.encoder.layer.8.attention.self.query.lora_A.lora_1.weight', 'roberta.encoder.layer.8.attention.self.query.lora_B.lora_1.weight', 'roberta.encoder.layer.8.attention.self.value.base_layer.weight', 'roberta.encoder.layer.8.attention.self.value.base_layer.bias', 'roberta.encoder.layer.8.attention.self.value.lora_A.lora_1.weight', 'roberta.encoder.layer.8.attention.self.value.lora_B.lora_1.weight', 'roberta.encoder.layer.9.attention.self.query.base_layer.weight', 'roberta.encoder.layer.9.attention.self.query.base_layer.bias', 'roberta.encoder.layer.9.attention.self.query.lora_A.lora_1.weight', 'roberta.encoder.layer.9.attention.self.query.lora_B.lora_1.weight', 'roberta.encoder.layer.9.attention.self.value.base_layer.weight', 'roberta.encoder.layer.9.attention.self.value.base_layer.bias', 'roberta.encoder.layer.9.attention.self.value.lora_A.lora_1.weight', 'roberta.encoder.layer.9.attention.self.value.lora_B.lora_1.weight', 'roberta.encoder.layer.10.attention.self.query.base_layer.weight', 'roberta.encoder.layer.10.attention.self.query.base_layer.bias', 'roberta.encoder.layer.10.attention.self.query.lora_A.lora_1.weight', 'roberta.encoder.layer.10.attention.self.query.lora_B.lora_1.weight', 'roberta.encoder.layer.10.attention.self.value.base_layer.weight', 'roberta.encoder.layer.10.attention.self.value.base_layer.bias', 'roberta.encoder.layer.10.attention.self.value.lora_A.lora_1.weight', 'roberta.encoder.layer.10.attention.self.value.lora_B.lora_1.weight', 'roberta.encoder.layer.11.attention.self.query.base_layer.weight', 'roberta.encoder.layer.11.attention.self.query.base_layer.bias', 'roberta.encoder.layer.11.attention.self.query.lora_A.lora_1.weight', 'roberta.encoder.layer.11.attention.self.query.lora_B.lora_1.weight', 'roberta.encoder.layer.11.attention.self.value.base_layer.weight', 'roberta.encoder.layer.11.attention.self.value.base_layer.bias', 'roberta.encoder.layer.11.attention.self.value.lora_A.lora_1.weight', 'roberta.encoder.layer.11.attention.self.value.lora_B.lora_1.weight'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.value.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:34, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.686005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.686001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.686002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.686008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.686009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.686008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.686010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.686009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.686002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.685358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=1.202075439453125, metrics={'train_runtime': 35.1997, 'train_samples_per_second': 53.978, 'train_steps_per_second': 7.102, 'total_flos': 250175158886400.0, 'train_loss': 1.202075439453125, 'epoch': 50.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.train(\n",
    "    resume_from_checkpoint=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1db0351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Algorithms/Logic', 0.4031808078289032)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_code(\"\"\" \n",
    "def post(request):\n",
    "    data = request.json()\n",
    "    user = data.get(\"user\")\n",
    "    return {\"message\": f\"Hello, {user}!\"}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd70f38",
   "metadata": {},
   "source": [
    "# LLM call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "914f57ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.102.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from openai) (4.10.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Downloading openai-1.102.0-py3-none-any.whl (812 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.0/812.0 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Installing collected packages: jiter, distro, openai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [openai]2m2/3\u001b[0m [openai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed distro-1.9.0 jiter-0.10.0 openai-1.102.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eca5d5f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'HF_TOKEN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      4\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[1;32m      5\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://router.huggingface.co/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 6\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHF_TOKEN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoonshotai/Kimi-K2-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     ],\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/os.py:680\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    677\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HF_TOKEN'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"moonshotai/Kimi-K2-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Complete this: \\\"You are gonna get <mask>, kid!\\\"\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd244b9",
   "metadata": {},
   "source": [
    "# Code classifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "\n",
    "class CodeClassifier:\n",
    "    def __init__(self, id2label, device):\n",
    "        self._model = None\n",
    "        self._tokenizer = None\n",
    "        self.id2label = id2label\n",
    "        self.device = device\n",
    "\n",
    "    def classify(self, code_snippet):\n",
    "        inputs = self._tokenizer(code_snippet, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "        outputs = self._model(**inputs.to(self.device))\n",
    "        probs = outputs.logits.softmax(dim=1)\n",
    "        pred = probs.argmax(dim=1).item()\n",
    "        return self.id2label[pred], probs[0][pred].item()\n",
    "    \n",
    "\n",
    "class CodeBERTClassifier(CodeClassifier):\n",
    "    def __init__(self, id2label, device, model_path=None):\n",
    "        super().__init__(id2label, device)\n",
    "\n",
    "        if model_path is None:\n",
    "            model_path = \"microsoft/codebert-base\"\n",
    "\n",
    "        self._model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_path,\n",
    "            id2label=id2label,\n",
    "            label2id={label: i for i, label in id2label.items()},\n",
    "        ).to(device)\n",
    "\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "classifier = CodeBERTClassifier(id2label, device, model_path=\"./codebert-finetuned/final_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
