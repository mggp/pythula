{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9cb0a18-b203-4e77-8058-d7ddf6ae6fdc",
   "metadata": {},
   "source": [
    "# Description\n",
    "The idea is to build a model that helps with classifying python code snippets.\n",
    "\n",
    "Its output is 1 out of 4 classes:\n",
    "- Data Processing\n",
    "- Web/API Code\n",
    "- Algorithms/Logic\n",
    "- Machine Learning\n",
    "\n",
    "# Dataset\n",
    "Ideas:\n",
    "- Use sth like `CodeSearchNet`\n",
    "- Scrape github repos and leverage info from their tags\n",
    "- Create a synthetic dataset from tutorials or documentation\n",
    "\n",
    "Goal: Have 100 to 500 data points per category.\n",
    "\n",
    "# Preprocessing\n",
    "- Use a tokenizer like HuggingFace's AutoTokenizer\n",
    "\n",
    "# Training\n",
    "- Fine-tune something like CodeBERT\n",
    "- Loss: CrossEntropyLoss\n",
    "- Optimizer: Adam\n",
    "- Epochs: 5–10 (for initial demo-level)\n",
    "\n",
    "## Evaluation\n",
    "- Accuracy\n",
    "- Confusion matrix\n",
    "\n",
    "# Questions:\n",
    "- How to store/load model?\n",
    "- How to compare models?\n",
    "- Baseline vs. mine vs. huggingface/CodeBERTa-small-v1 vs. DistilBERT vs. ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a41cfb8a-2e95-4bb7-bbe6-099624e2e7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72931627-8ea3-4bc3-96be-1ace313e86f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5286beaf-bec5-47d9-ba69-8797b5fc4d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 9232,\n",
       " 19220,\n",
       " 1640,\n",
       " 102,\n",
       " 6,\n",
       " 428,\n",
       " 3256,\n",
       " 114,\n",
       " 10,\n",
       " 15698,\n",
       " 428,\n",
       " 35,\n",
       " 671,\n",
       " 10,\n",
       " 1493,\n",
       " 671,\n",
       " 741,\n",
       " 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_tokens = tokenizer.tokenize(\"def max(a,b): if a>b: return a else return b\")\n",
    "tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.eos_token]\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa42ca4b-d5a6-4e5a-9ccf-92d144a661a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1685,  0.3331,  0.0392,  ..., -0.2262, -0.3359,  0.3277],\n",
       "         [-1.0436,  0.3191,  0.3959,  ..., -0.4708, -0.1289,  0.5579],\n",
       "         [-0.9022,  0.5009,  0.1820,  ..., -0.4935, -0.5855,  0.6971],\n",
       "         ...,\n",
       "         [-0.4663,  0.2088,  0.5154,  ..., -0.1752, -0.3702,  0.5890],\n",
       "         [-0.4513,  0.4893,  0.4857,  ..., -0.3150, -0.6229,  0.3867],\n",
       "         [-0.1703,  0.3353,  0.0404,  ..., -0.2282, -0.3384,  0.3300]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 4.9750e-01, -3.6809e-01, -5.7454e-01,  1.0966e-01,  3.4282e-01,\n",
       "          7.0422e-02,  4.8956e-01, -3.5818e-01,  1.0147e-01, -3.2847e-01,\n",
       "          4.4498e-01,  2.5206e-02, -2.6024e-01,  1.1288e-01, -6.0998e-02,\n",
       "          5.9084e-01,  4.4048e-01, -5.2328e-01,  9.5334e-02,  3.2977e-01,\n",
       "         -3.4737e-01,  5.2274e-01,  3.5409e-01,  8.1222e-04, -4.9782e-02,\n",
       "          2.3763e-01,  1.2317e-01,  9.8236e-02,  4.9072e-01,  1.4917e-01,\n",
       "          1.0529e-01,  1.1462e-01,  2.6017e-01, -3.1969e-02, -3.4618e-01,\n",
       "         -5.9457e-02, -5.2262e-01,  1.7050e-01,  6.8206e-01, -3.1251e-01,\n",
       "         -3.8177e-01,  4.8588e-02, -1.5073e-02, -3.5364e-01,  9.7735e-02,\n",
       "          6.2407e-01,  1.7006e-01,  5.8128e-02, -2.4977e-01, -2.6303e-01,\n",
       "         -4.8945e-01,  4.2905e-01,  3.7614e-01,  1.8760e-01, -2.7240e-01,\n",
       "          1.2351e-01,  5.3280e-02, -1.4142e-01, -2.2490e-02, -3.2160e-01,\n",
       "         -4.2335e-01, -4.4173e-01,  6.4758e-02,  1.9320e-01, -1.0279e-01,\n",
       "         -1.9797e-01,  3.9336e-01,  1.7219e-02, -3.4886e-01,  4.3432e-02,\n",
       "         -3.0433e-01,  1.7277e-01,  2.3118e-03, -7.4777e-01, -1.1275e-01,\n",
       "          6.1676e-02, -6.1766e-01,  8.6231e-02,  6.2932e-01,  4.2631e-01,\n",
       "         -7.6890e-02,  3.5614e-01, -1.7805e-02,  2.9910e-01, -1.6042e-01,\n",
       "         -3.1510e-01,  5.1284e-01, -1.3771e-01,  1.5109e-01,  3.4099e-01,\n",
       "         -3.5993e-01, -6.8903e-01, -1.4666e-01,  1.5198e-01, -2.8632e-01,\n",
       "          1.9759e-01, -3.1253e-02,  1.1719e-01, -3.7307e-01, -6.5032e-02,\n",
       "          2.1109e-01, -2.4931e-01, -8.9592e-02,  2.4239e-01,  3.1544e-01,\n",
       "         -2.3738e-01, -2.3566e-01,  6.7964e-02,  1.1827e-01, -1.7191e-01,\n",
       "         -1.8114e-01,  6.3513e-01,  5.4469e-01,  1.8282e-01,  1.4556e-01,\n",
       "          1.0282e-01, -2.4123e-01, -3.6399e-01,  4.1590e-01, -4.0026e-01,\n",
       "          2.0248e-01, -1.6243e-01,  1.0541e-01,  3.1628e-01, -3.7827e-01,\n",
       "          2.0589e-01,  9.8366e-02,  4.7112e-01,  2.0192e-01, -1.0810e-01,\n",
       "         -2.5779e-02, -4.3047e-02, -6.2478e-02,  2.0629e-01, -1.1479e-02,\n",
       "          7.9146e-02,  1.6876e-01, -7.4072e-01, -3.9138e-01,  5.6004e-01,\n",
       "          7.8801e-01,  9.8383e-02,  2.1465e-01,  2.1456e-01,  4.9916e-01,\n",
       "          5.9873e-01,  2.7066e-01, -5.6034e-01,  7.7182e-04,  3.2314e-01,\n",
       "         -8.4422e-02, -9.8721e-02, -2.8894e-01, -5.1492e-01, -6.2231e-01,\n",
       "         -7.1195e-02,  3.7135e-01, -2.2594e-02,  8.9180e-03,  5.8837e-01,\n",
       "          2.3804e-01, -4.3125e-01, -2.1761e-01, -1.8206e-01, -1.9061e-01,\n",
       "         -4.0133e-01, -1.6558e-01, -3.6317e-02, -4.3803e-01, -4.5489e-01,\n",
       "         -1.2832e-02, -5.5757e-01, -3.4004e-02,  3.1681e-01, -5.2866e-01,\n",
       "          6.0848e-01, -5.4178e-01,  1.3073e-01,  5.0709e-01, -4.0953e-01,\n",
       "          1.1111e-01, -4.9435e-01,  6.8678e-02,  4.0986e-01,  9.3627e-02,\n",
       "          3.1640e-01, -2.8635e-01,  4.7105e-01, -4.5707e-02,  2.1905e-01,\n",
       "          2.6631e-01,  1.4883e-02, -3.2077e-01,  3.5038e-01, -4.5549e-01,\n",
       "          2.2594e-01, -3.8315e-01, -1.2406e-01, -3.8408e-01, -3.3107e-01,\n",
       "          1.9691e-01, -7.9483e-01, -5.7436e-01,  6.0311e-02, -1.3771e-01,\n",
       "          3.1008e-02, -7.0998e-02,  3.6987e-02,  5.6002e-02, -1.3018e-01,\n",
       "          2.8744e-02, -4.5530e-01,  3.7241e-01, -2.9324e-01, -2.2123e-01,\n",
       "         -6.2677e-02,  3.1586e-01,  4.1470e-01,  2.2050e-01, -5.3521e-01,\n",
       "         -3.7554e-01,  1.9667e-01,  4.7342e-01, -1.6239e-01,  5.9071e-01,\n",
       "         -2.0297e-01, -2.5248e-01,  4.9344e-03,  3.1409e-01,  1.5819e-01,\n",
       "          5.3888e-01, -3.3149e-01, -4.4422e-02,  6.7460e-02, -5.3586e-01,\n",
       "         -3.1176e-01, -2.4766e-01,  2.4565e-01,  4.4887e-01,  4.8352e-02,\n",
       "          1.8375e-01,  4.1069e-01,  3.0484e-01,  9.3303e-03,  4.3717e-01,\n",
       "         -1.8158e-01,  2.3197e-01, -4.8631e-01, -4.8303e-02, -4.4866e-01,\n",
       "         -3.8262e-01, -4.7719e-01,  7.2491e-01, -3.2524e-01,  4.9916e-01,\n",
       "          5.2499e-01, -3.6963e-01, -2.3948e-01,  2.6310e-01,  1.5191e-01,\n",
       "          1.6487e-01, -2.7358e-02,  4.3545e-02,  2.1092e-01,  8.1634e-02,\n",
       "          3.5018e-01,  5.5283e-01,  2.3565e-01,  3.7428e-01, -4.7256e-03,\n",
       "          4.6579e-02,  4.0812e-01, -1.7120e-01, -1.8692e-01, -4.0550e-02,\n",
       "          1.7061e-01, -2.3378e-01, -4.1320e-01,  2.6783e-04, -3.6651e-02,\n",
       "          6.2209e-01, -1.1946e-01, -3.6498e-01,  2.2925e-01,  2.6719e-01,\n",
       "         -6.2986e-01,  1.2402e-01,  1.7832e-01,  2.0149e-01, -5.0035e-01,\n",
       "         -1.0863e-01,  3.3841e-02,  2.1838e-01, -5.4170e-01, -4.6367e-01,\n",
       "          5.6931e-01, -9.0834e-04,  2.9940e-01,  2.3746e-01, -3.1228e-01,\n",
       "         -2.8088e-01,  7.6634e-01, -1.2276e-01, -4.7541e-01,  3.5624e-01,\n",
       "          1.6490e-01,  4.0736e-03,  2.5346e-01,  1.7811e-01,  3.6203e-01,\n",
       "         -2.7857e-01,  5.0861e-01,  2.2278e-01, -6.1800e-01, -4.1455e-01,\n",
       "         -2.4309e-01, -9.8205e-04, -1.2631e-02, -3.2109e-01, -5.2967e-01,\n",
       "          8.6065e-02,  2.2637e-01, -2.0267e-01,  3.2809e-01,  2.1909e-01,\n",
       "          3.5374e-01, -5.1346e-02,  4.7302e-01, -2.8101e-01,  5.3436e-01,\n",
       "         -5.8113e-02,  5.5476e-01, -5.1958e-01, -8.1967e-02, -7.1035e-02,\n",
       "         -1.8847e-01, -4.9061e-02,  2.7026e-02,  3.5189e-01, -2.7104e-01,\n",
       "         -5.6880e-01,  2.3940e-01,  2.5550e-01,  1.7921e-01,  2.8563e-01,\n",
       "          4.4189e-01,  1.6059e-01,  1.6586e-01,  3.7000e-02,  2.8754e-01,\n",
       "          2.7540e-01, -2.4854e-01, -6.8635e-01,  2.9518e-01, -4.5267e-01,\n",
       "         -6.4859e-02, -2.4385e-01, -2.9751e-01,  6.9105e-01, -2.8178e-01,\n",
       "          2.4190e-01,  3.9338e-01,  2.7151e-01,  2.4004e-01,  5.1353e-02,\n",
       "          2.7501e-01,  2.7901e-01,  1.4271e-01,  9.9427e-02, -1.6383e-02,\n",
       "         -3.7496e-01, -3.1621e-01, -2.0294e-01,  2.2598e-01, -3.4380e-01,\n",
       "         -5.2909e-01,  1.1344e-01,  5.0209e-01,  1.1508e-01, -3.4032e-01,\n",
       "          3.4861e-01,  3.5542e-01,  1.7158e-01,  1.0434e-02, -2.2489e-01,\n",
       "         -5.4942e-02,  6.1403e-01, -1.0975e-01,  1.7118e-01,  7.3632e-01,\n",
       "          2.5192e-01, -5.3250e-01, -1.2624e-01, -2.7634e-01,  1.3144e-01,\n",
       "          1.8390e-01, -3.1698e-01,  2.5134e-01,  4.8554e-01,  3.0017e-01,\n",
       "          7.3450e-01,  5.8561e-02,  1.0687e-01,  1.0761e-01,  2.4819e-01,\n",
       "         -1.0415e-01,  1.1396e-01,  6.4847e-02,  5.1507e-01,  4.5176e-01,\n",
       "         -3.9162e-01,  5.6699e-02, -2.1709e-01, -8.5214e-02, -1.7616e-01,\n",
       "         -4.7523e-01, -1.2134e-02,  3.3794e-01, -5.7909e-01,  6.4076e-02,\n",
       "         -2.1752e-01,  1.1341e-01, -8.7520e-02, -1.0765e-01, -2.3029e-01,\n",
       "         -4.0820e-01,  6.8080e-01, -3.9544e-02, -1.0757e-01, -4.4884e-01,\n",
       "         -4.0001e-01,  6.1110e-02,  2.0842e-01, -3.3845e-01, -2.4540e-01,\n",
       "          4.8759e-01, -6.4019e-02, -5.7059e-02, -1.6874e-01,  2.4369e-01,\n",
       "         -2.4259e-01,  2.5890e-01,  2.6719e-01, -2.1164e-01, -2.5709e-01,\n",
       "          7.0956e-02, -4.0929e-01, -3.3671e-01, -4.9956e-01,  5.5042e-01,\n",
       "         -3.4912e-01, -2.0008e-01, -2.3730e-01, -6.1538e-01,  1.3825e-01,\n",
       "          3.2920e-01,  5.2705e-01, -3.1624e-01,  1.0704e-01,  7.8889e-01,\n",
       "         -9.5615e-02, -3.5749e-01,  1.1383e-02,  3.4800e-01, -7.4367e-02,\n",
       "          6.4183e-01,  5.2562e-01, -2.2438e-02,  1.6308e-01,  7.2264e-01,\n",
       "          5.2338e-02,  1.9646e-01, -2.9163e-01,  5.3137e-01, -8.5046e-02,\n",
       "          2.9320e-01,  2.5419e-02,  5.0183e-02, -4.6959e-03, -1.2442e-01,\n",
       "          3.9640e-01,  4.8759e-01, -6.8770e-01, -2.0909e-01,  1.7391e-01,\n",
       "          4.1043e-02, -2.0719e-01, -4.2706e-01, -1.7395e-01, -3.4407e-01,\n",
       "         -1.2792e-01, -1.5234e-01, -7.6935e-02, -4.8482e-01, -1.4220e-01,\n",
       "          3.1179e-01, -4.3819e-01,  1.5216e-01,  6.0492e-01, -2.0609e-02,\n",
       "          3.0679e-01, -4.0934e-01, -8.8663e-02,  3.6965e-01, -1.8421e-01,\n",
       "         -3.6191e-01,  1.8609e-01,  8.4222e-01, -2.4798e-01, -7.3185e-01,\n",
       "         -6.7578e-02,  3.7685e-01,  6.6734e-02,  2.0544e-01, -1.0685e-01,\n",
       "         -3.0224e-01,  8.9721e-02, -9.9897e-04,  8.0792e-02, -2.6428e-01,\n",
       "         -5.8369e-01, -2.6540e-01,  5.9132e-01, -6.0704e-01,  8.4323e-02,\n",
       "         -1.7520e-01, -1.2734e-02, -4.6975e-01,  2.3337e-01, -3.4770e-01,\n",
       "          6.6131e-01,  1.6826e-01, -5.7643e-01,  1.0542e-01,  6.1783e-02,\n",
       "         -7.7056e-02, -1.7524e-01, -2.0857e-01,  7.0767e-01, -3.6453e-01,\n",
       "         -8.0199e-01,  1.8301e-01,  2.3899e-01,  6.3045e-01, -2.8039e-01,\n",
       "         -5.5762e-02, -2.3400e-01,  2.3956e-01,  1.8500e-01,  6.3159e-03,\n",
       "         -2.2380e-01, -1.7693e-01, -5.6956e-01, -3.7252e-01, -5.2656e-01,\n",
       "          1.0625e-01,  1.3280e-01, -5.8075e-01,  2.6341e-01, -1.4378e-01,\n",
       "          3.2977e-01,  2.0335e-01, -3.7786e-01, -1.0401e-01,  4.0183e-01,\n",
       "          4.7947e-01,  2.4861e-01,  4.9808e-01,  2.7402e-01,  2.3218e-01,\n",
       "         -2.4913e-01,  1.5188e-01,  1.7746e-01, -1.2191e-01,  4.9082e-01,\n",
       "         -1.4955e-01, -6.2619e-01, -1.0400e-01,  7.2152e-01,  9.9393e-02,\n",
       "         -3.5058e-01, -7.9178e-03,  5.9675e-01,  2.0104e-01,  7.0921e-02,\n",
       "          3.0117e-01, -4.0452e-01, -2.3563e-01, -7.2239e-02, -7.9153e-02,\n",
       "         -4.4002e-01, -2.4151e-01, -7.3683e-02, -1.5744e-01, -2.9276e-01,\n",
       "         -9.0456e-02, -2.2447e-01,  3.8692e-01, -5.8112e-01, -8.5422e-02,\n",
       "         -1.2147e-01, -9.2773e-02, -2.6203e-01,  1.5819e-01,  1.9313e-01,\n",
       "          3.7981e-02,  9.2126e-02,  6.3672e-01, -1.1875e-01, -1.6367e-01,\n",
       "         -2.0311e-01, -2.3932e-01,  2.7866e-01, -3.2459e-01,  9.4900e-02,\n",
       "         -1.0008e-01,  3.1609e-01, -5.9932e-01, -2.7006e-01,  7.5978e-04,\n",
       "         -2.6134e-01, -4.1266e-01,  4.4862e-01,  2.6931e-01,  2.1707e-01,\n",
       "          5.3670e-01,  9.6922e-02,  5.2163e-02, -3.3105e-01, -3.8879e-01,\n",
       "         -2.5432e-01,  2.3009e-01, -7.5707e-03, -4.7482e-01, -2.0281e-01,\n",
       "          2.3660e-01, -4.9760e-01, -2.5120e-01,  3.5086e-01, -9.4184e-04,\n",
       "         -1.8856e-01, -2.3253e-01, -1.5125e-01, -6.6113e-01,  1.6864e-01,\n",
       "          1.0934e-01,  7.1617e-02, -2.2990e-01,  1.0219e-01, -2.2715e-01,\n",
       "          1.8264e-01,  2.5516e-01,  9.1116e-02, -1.7003e-01, -4.3305e-01,\n",
       "         -4.9145e-01, -3.2139e-01,  5.7903e-02,  3.6657e-01, -2.2949e-01,\n",
       "         -2.8063e-01,  1.1617e-01,  3.6120e-01, -1.7741e-01,  1.1957e-01,\n",
       "          1.7351e-01, -6.6759e-01, -2.1306e-01, -4.7903e-02,  1.0976e-01,\n",
       "         -1.6574e-02, -2.5477e-01, -4.2668e-01,  3.1086e-01, -7.8112e-02,\n",
       "         -2.5410e-01,  5.9363e-01, -2.3090e-01,  4.0536e-01, -3.1344e-02,\n",
       "         -3.6027e-01, -1.7356e-01,  4.4256e-01, -2.4134e-02,  3.2344e-01,\n",
       "          1.2147e-01, -5.6970e-01, -6.8544e-02, -3.5827e-02, -2.9050e-01,\n",
       "         -2.6791e-01, -1.2770e-01, -1.2066e-01,  3.3045e-01, -6.2850e-01,\n",
       "          4.3790e-01,  8.3780e-02,  2.0555e-01,  2.2921e-01, -4.3751e-01,\n",
       "         -2.8730e-01,  2.6541e-01,  3.6385e-01, -2.7253e-01, -5.5142e-01,\n",
       "         -4.3237e-01, -3.5846e-01, -3.5458e-01, -2.6223e-01,  5.9106e-01,\n",
       "         -9.3076e-02, -2.7750e-01, -2.3211e-01,  5.5739e-01,  1.2421e-01,\n",
       "          1.8462e-03,  4.1556e-01,  1.6861e-01,  1.6488e-01,  2.2025e-01,\n",
       "         -7.3772e-01,  2.5997e-01, -3.9701e-01,  4.5173e-03, -8.4839e-02,\n",
       "          2.6684e-01, -3.0623e-01,  3.8680e-02, -3.4479e-01,  1.2279e-01,\n",
       "          3.9140e-01, -4.0717e-01, -1.4384e-01,  3.5290e-01,  2.1646e-01,\n",
       "         -3.4454e-01,  5.9429e-02,  1.9127e-01,  3.4930e-01,  1.3720e-01,\n",
       "          1.0680e-01,  4.4797e-01, -4.7895e-01,  3.2598e-03, -4.5932e-01,\n",
       "         -5.6631e-01,  2.2528e-01,  1.3527e-01,  3.9478e-01, -1.3899e-01,\n",
       "         -3.7747e-01,  6.4856e-01,  5.4442e-02, -7.5439e-02,  2.8162e-01,\n",
       "         -8.9302e-03, -5.4320e-02, -2.5094e-01,  1.9559e-01,  6.2133e-01,\n",
       "         -6.4973e-02,  9.0919e-02,  4.4535e-01, -5.6958e-01,  3.5233e-01,\n",
       "         -1.3435e-01, -1.8565e-02,  8.9066e-02]], device='cuda:0',\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(token_ids).to(device)[None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10318794-379b-437d-9627-f8cc875dc16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "sample_code = \"def max(a,b): if a>b: return a else return b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aa8cb9e-f189-4c10-a2fd-061a5b91e481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 9232, 19220, 1640, 102, 6, 428, 3256, 114, 10, 15698, 428, 35, 671, 10, 1493, 671, 741, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sample_code, padding=\"max_length\", truncation=True, max_length=256# # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e786d42-887f-475c-ad55-2cb9935b9a89",
   "metadata": {},
   "source": [
    "# Fine-tuning v0\n",
    "## Create dataset\n",
    "This also creates the label2id and id2label maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4c2125d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"Data Processing\", \"Web/API Code\", \"Algorithms/Logic\", \"Machine Learning\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9a30fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"data/balanced_data.jsonl\", split=\"train\")\n",
    "dataset = dataset.map(lambda x: {\"label\": label2id[x[\"label\"]]})\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc81e3-5b54-4091-a659-7300370edd6b",
   "metadata": {},
   "source": [
    "## Create tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a4e46def-4ace-4e7f-994c-2c71d441ab2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea008a023a154f388a2fa0cb07e2a46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c33fcba65364e23a57882edc9786a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"code\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d1cbe06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['code', 'label', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 36\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['code', 'label', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 9\n",
       "     })\n",
       " }),\n",
       " RobertaTokenizerFast(name_or_path='microsoft/codebert-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       " \t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       " }\n",
       " ))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c62cbeb-84b0-4e19-bad2-96babc09a37c",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b98fb62c-22bf-43e0-ad38-e65e378f8230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/codebert-base\",\n",
    "    num_labels=4,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e2eff3-123a-4f1a-b7a5-25d1bd89be67",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1030af7c-11e6-4494-b84c-48a504152d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./codebert-finetuned\",\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_on_start=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3bdf5543-4cb7-4a1b-afb4-4f04dd79cf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_580285/1908840112.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5fcfea27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1917643547058105,\n",
       " 'eval_model_preparation_time': 0.0036,\n",
       " 'eval_runtime': 0.1914,\n",
       " 'eval_samples_per_second': 47.03,\n",
       " 'eval_steps_per_second': 5.226}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c34ec-8998-4b47-babd-6e2ec3c3a977",
   "metadata": {},
   "source": [
    "## Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7dff51ae-4ceb-4377-9d4a-dd697d855d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_code(code_snippet):\n",
    "    inputs = tokenizer(code_snippet, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    outputs = model(**inputs.to(device))\n",
    "    probs = outputs.logits.softmax(dim=1)\n",
    "    pred = probs.argmax(dim=1).item()\n",
    "    return id2label[pred], probs[0][pred].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8d55569-b7e5-4bf0-8e7d-853de701964f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Data Processing', 0.335357129573822)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_code = \"\"\"\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\"\"\"\n",
    "classify_code(sample_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf35a5d-f5cc-4c61-a1ed-cec65aa40682",
   "metadata": {},
   "source": [
    "Estaría bueno que el scraper acumule en lugar de borrar. Que se guarde el hash para no duplicar cosas y que haya un sistema de reviews. Pienso que estaría genial que muestre el código formateado, que haya un botón para aprobar y pasar al siguiente o reprobar y hacer un soft delete antes de avanazar. Tiene que ser un soft delete para que no se vuelvan a agregar y revisar códigos ya revisados. \n",
    "\n",
    "Quizá incluso estaría bueno que no se procese dos veces la misma página. Que no se guarde sólo un pedazo de código (o su hash), sino una URI. Y que no se tomen dos pedazos de código del mismo lugar. Así, la diversidad será máxima.\n",
    "\n",
    "Pero todo esto puede quedar para una segunda revisión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb9d70f",
   "metadata": {},
   "source": [
    "# Migrar a pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6fb34f",
   "metadata": {},
   "source": [
    "# Limpiar dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61582f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3 as sqlite\n",
    "import json\n",
    "\n",
    "db_path = \"schemas/code_snippets.db\"\n",
    "random_seed = 42\n",
    "\n",
    "conn = sqlite.connect(db_path)\n",
    "df = pd.read_sql(\"SELECT * FROM snippets ORDER BY hash\", conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc06f2",
   "metadata": {},
   "source": [
    "Es probable que esto quede desbalanceado. Hay que asegurarse que no sea el caso en el dataset de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "99829eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Se tomarán 8 ejemplos por clase para balancear.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "Web/API Code        13\n",
       "Data Processing     13\n",
       "Algorithms/Logic     9\n",
       "Machine Learning     8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = df[\"label\"].value_counts()\n",
    "min_count = counts.min()\n",
    "\n",
    "print(f\"\\nSe tomarán {min_count} ejemplos por clase para balancear.\")\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52403d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "balanced = (\n",
    "    df.groupby(\"label\", group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=min_count, random_state=random_seed))\n",
    "    .reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e848a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_file = \"data/balanced_data.jsonl\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in balanced.iterrows():\n",
    "        record = {\n",
    "            \"code\": row[\"code\"],\n",
    "            \"label\": row[\"label\"]\n",
    "        }\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
