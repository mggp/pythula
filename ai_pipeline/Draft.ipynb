{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9cb0a18-b203-4e77-8058-d7ddf6ae6fdc",
   "metadata": {},
   "source": [
    "# Description\n",
    "The idea is to build a model that helps with classifying python code snippets.\n",
    "\n",
    "Its output is 1 out of 4 classes:\n",
    "- Data Processing\n",
    "- Web/API Code\n",
    "- Algorithms/Logic\n",
    "- Machine Learning\n",
    "\n",
    "# Dataset\n",
    "Ideas:\n",
    "- Use sth like `CodeSearchNet`\n",
    "- Scrape github repos and leverage info from their tags\n",
    "- Create a synthetic dataset from tutorials or documentation\n",
    "\n",
    "Goal: Have 100 to 500 data points per category.\n",
    "\n",
    "# Preprocessing\n",
    "- Use a tokenizer like HuggingFace's AutoTokenizer\n",
    "\n",
    "# Training\n",
    "- Fine-tune something like CodeBERT\n",
    "- Loss: CrossEntropyLoss\n",
    "- Optimizer: Adam\n",
    "- Epochs: 5–10 (for initial demo-level)\n",
    "\n",
    "## Evaluation\n",
    "- Accuracy\n",
    "- Confusion matrix\n",
    "\n",
    "# Questions:\n",
    "- How to store/load model?\n",
    "- How to compare models?\n",
    "- Baseline vs. mine vs. huggingface/CodeBERTa-small-v1 vs. DistilBERT vs. ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a41cfb8a-2e95-4bb7-bbe6-099624e2e7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72931627-8ea3-4bc3-96be-1ace313e86f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5286beaf-bec5-47d9-ba69-8797b5fc4d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 9232,\n",
       " 19220,\n",
       " 1640,\n",
       " 102,\n",
       " 6,\n",
       " 428,\n",
       " 3256,\n",
       " 114,\n",
       " 10,\n",
       " 15698,\n",
       " 428,\n",
       " 35,\n",
       " 671,\n",
       " 10,\n",
       " 1493,\n",
       " 671,\n",
       " 741,\n",
       " 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_tokens = tokenizer.tokenize(\"def max(a,b): if a>b: return a else return b\")\n",
    "tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.eos_token]\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa42ca4b-d5a6-4e5a-9ccf-92d144a661a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1685,  0.3331,  0.0392,  ..., -0.2262, -0.3359,  0.3277],\n",
       "         [-1.0436,  0.3191,  0.3959,  ..., -0.4708, -0.1289,  0.5579],\n",
       "         [-0.9022,  0.5009,  0.1820,  ..., -0.4935, -0.5855,  0.6971],\n",
       "         ...,\n",
       "         [-0.4663,  0.2088,  0.5154,  ..., -0.1752, -0.3702,  0.5890],\n",
       "         [-0.4513,  0.4893,  0.4857,  ..., -0.3150, -0.6229,  0.3867],\n",
       "         [-0.1703,  0.3353,  0.0404,  ..., -0.2282, -0.3384,  0.3300]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 4.9750e-01, -3.6809e-01, -5.7454e-01,  1.0966e-01,  3.4282e-01,\n",
       "          7.0422e-02,  4.8956e-01, -3.5818e-01,  1.0147e-01, -3.2847e-01,\n",
       "          4.4498e-01,  2.5206e-02, -2.6024e-01,  1.1288e-01, -6.0998e-02,\n",
       "          5.9084e-01,  4.4048e-01, -5.2328e-01,  9.5334e-02,  3.2977e-01,\n",
       "         -3.4737e-01,  5.2274e-01,  3.5409e-01,  8.1222e-04, -4.9782e-02,\n",
       "          2.3763e-01,  1.2317e-01,  9.8236e-02,  4.9072e-01,  1.4917e-01,\n",
       "          1.0529e-01,  1.1462e-01,  2.6017e-01, -3.1969e-02, -3.4618e-01,\n",
       "         -5.9457e-02, -5.2262e-01,  1.7050e-01,  6.8206e-01, -3.1251e-01,\n",
       "         -3.8177e-01,  4.8588e-02, -1.5073e-02, -3.5364e-01,  9.7735e-02,\n",
       "          6.2407e-01,  1.7006e-01,  5.8128e-02, -2.4977e-01, -2.6303e-01,\n",
       "         -4.8945e-01,  4.2905e-01,  3.7614e-01,  1.8760e-01, -2.7240e-01,\n",
       "          1.2351e-01,  5.3280e-02, -1.4142e-01, -2.2490e-02, -3.2160e-01,\n",
       "         -4.2335e-01, -4.4173e-01,  6.4758e-02,  1.9320e-01, -1.0279e-01,\n",
       "         -1.9797e-01,  3.9336e-01,  1.7219e-02, -3.4886e-01,  4.3432e-02,\n",
       "         -3.0433e-01,  1.7277e-01,  2.3118e-03, -7.4777e-01, -1.1275e-01,\n",
       "          6.1676e-02, -6.1766e-01,  8.6231e-02,  6.2932e-01,  4.2631e-01,\n",
       "         -7.6890e-02,  3.5614e-01, -1.7805e-02,  2.9910e-01, -1.6042e-01,\n",
       "         -3.1510e-01,  5.1284e-01, -1.3771e-01,  1.5109e-01,  3.4099e-01,\n",
       "         -3.5993e-01, -6.8903e-01, -1.4666e-01,  1.5198e-01, -2.8632e-01,\n",
       "          1.9759e-01, -3.1253e-02,  1.1719e-01, -3.7307e-01, -6.5032e-02,\n",
       "          2.1109e-01, -2.4931e-01, -8.9592e-02,  2.4239e-01,  3.1544e-01,\n",
       "         -2.3738e-01, -2.3566e-01,  6.7964e-02,  1.1827e-01, -1.7191e-01,\n",
       "         -1.8114e-01,  6.3513e-01,  5.4469e-01,  1.8282e-01,  1.4556e-01,\n",
       "          1.0282e-01, -2.4123e-01, -3.6399e-01,  4.1590e-01, -4.0026e-01,\n",
       "          2.0248e-01, -1.6243e-01,  1.0541e-01,  3.1628e-01, -3.7827e-01,\n",
       "          2.0589e-01,  9.8366e-02,  4.7112e-01,  2.0192e-01, -1.0810e-01,\n",
       "         -2.5779e-02, -4.3047e-02, -6.2478e-02,  2.0629e-01, -1.1479e-02,\n",
       "          7.9146e-02,  1.6876e-01, -7.4072e-01, -3.9138e-01,  5.6004e-01,\n",
       "          7.8801e-01,  9.8383e-02,  2.1465e-01,  2.1456e-01,  4.9916e-01,\n",
       "          5.9873e-01,  2.7066e-01, -5.6034e-01,  7.7182e-04,  3.2314e-01,\n",
       "         -8.4422e-02, -9.8721e-02, -2.8894e-01, -5.1492e-01, -6.2231e-01,\n",
       "         -7.1195e-02,  3.7135e-01, -2.2594e-02,  8.9180e-03,  5.8837e-01,\n",
       "          2.3804e-01, -4.3125e-01, -2.1761e-01, -1.8206e-01, -1.9061e-01,\n",
       "         -4.0133e-01, -1.6558e-01, -3.6317e-02, -4.3803e-01, -4.5489e-01,\n",
       "         -1.2832e-02, -5.5757e-01, -3.4004e-02,  3.1681e-01, -5.2866e-01,\n",
       "          6.0848e-01, -5.4178e-01,  1.3073e-01,  5.0709e-01, -4.0953e-01,\n",
       "          1.1111e-01, -4.9435e-01,  6.8678e-02,  4.0986e-01,  9.3627e-02,\n",
       "          3.1640e-01, -2.8635e-01,  4.7105e-01, -4.5707e-02,  2.1905e-01,\n",
       "          2.6631e-01,  1.4883e-02, -3.2077e-01,  3.5038e-01, -4.5549e-01,\n",
       "          2.2594e-01, -3.8315e-01, -1.2406e-01, -3.8408e-01, -3.3107e-01,\n",
       "          1.9691e-01, -7.9483e-01, -5.7436e-01,  6.0311e-02, -1.3771e-01,\n",
       "          3.1008e-02, -7.0998e-02,  3.6987e-02,  5.6002e-02, -1.3018e-01,\n",
       "          2.8744e-02, -4.5530e-01,  3.7241e-01, -2.9324e-01, -2.2123e-01,\n",
       "         -6.2677e-02,  3.1586e-01,  4.1470e-01,  2.2050e-01, -5.3521e-01,\n",
       "         -3.7554e-01,  1.9667e-01,  4.7342e-01, -1.6239e-01,  5.9071e-01,\n",
       "         -2.0297e-01, -2.5248e-01,  4.9344e-03,  3.1409e-01,  1.5819e-01,\n",
       "          5.3888e-01, -3.3149e-01, -4.4422e-02,  6.7460e-02, -5.3586e-01,\n",
       "         -3.1176e-01, -2.4766e-01,  2.4565e-01,  4.4887e-01,  4.8352e-02,\n",
       "          1.8375e-01,  4.1069e-01,  3.0484e-01,  9.3303e-03,  4.3717e-01,\n",
       "         -1.8158e-01,  2.3197e-01, -4.8631e-01, -4.8303e-02, -4.4866e-01,\n",
       "         -3.8262e-01, -4.7719e-01,  7.2491e-01, -3.2524e-01,  4.9916e-01,\n",
       "          5.2499e-01, -3.6963e-01, -2.3948e-01,  2.6310e-01,  1.5191e-01,\n",
       "          1.6487e-01, -2.7358e-02,  4.3545e-02,  2.1092e-01,  8.1634e-02,\n",
       "          3.5018e-01,  5.5283e-01,  2.3565e-01,  3.7428e-01, -4.7256e-03,\n",
       "          4.6579e-02,  4.0812e-01, -1.7120e-01, -1.8692e-01, -4.0550e-02,\n",
       "          1.7061e-01, -2.3378e-01, -4.1320e-01,  2.6783e-04, -3.6651e-02,\n",
       "          6.2209e-01, -1.1946e-01, -3.6498e-01,  2.2925e-01,  2.6719e-01,\n",
       "         -6.2986e-01,  1.2402e-01,  1.7832e-01,  2.0149e-01, -5.0035e-01,\n",
       "         -1.0863e-01,  3.3841e-02,  2.1838e-01, -5.4170e-01, -4.6367e-01,\n",
       "          5.6931e-01, -9.0834e-04,  2.9940e-01,  2.3746e-01, -3.1228e-01,\n",
       "         -2.8088e-01,  7.6634e-01, -1.2276e-01, -4.7541e-01,  3.5624e-01,\n",
       "          1.6490e-01,  4.0736e-03,  2.5346e-01,  1.7811e-01,  3.6203e-01,\n",
       "         -2.7857e-01,  5.0861e-01,  2.2278e-01, -6.1800e-01, -4.1455e-01,\n",
       "         -2.4309e-01, -9.8205e-04, -1.2631e-02, -3.2109e-01, -5.2967e-01,\n",
       "          8.6065e-02,  2.2637e-01, -2.0267e-01,  3.2809e-01,  2.1909e-01,\n",
       "          3.5374e-01, -5.1346e-02,  4.7302e-01, -2.8101e-01,  5.3436e-01,\n",
       "         -5.8113e-02,  5.5476e-01, -5.1958e-01, -8.1967e-02, -7.1035e-02,\n",
       "         -1.8847e-01, -4.9061e-02,  2.7026e-02,  3.5189e-01, -2.7104e-01,\n",
       "         -5.6880e-01,  2.3940e-01,  2.5550e-01,  1.7921e-01,  2.8563e-01,\n",
       "          4.4189e-01,  1.6059e-01,  1.6586e-01,  3.7000e-02,  2.8754e-01,\n",
       "          2.7540e-01, -2.4854e-01, -6.8635e-01,  2.9518e-01, -4.5267e-01,\n",
       "         -6.4859e-02, -2.4385e-01, -2.9751e-01,  6.9105e-01, -2.8178e-01,\n",
       "          2.4190e-01,  3.9338e-01,  2.7151e-01,  2.4004e-01,  5.1353e-02,\n",
       "          2.7501e-01,  2.7901e-01,  1.4271e-01,  9.9427e-02, -1.6383e-02,\n",
       "         -3.7496e-01, -3.1621e-01, -2.0294e-01,  2.2598e-01, -3.4380e-01,\n",
       "         -5.2909e-01,  1.1344e-01,  5.0209e-01,  1.1508e-01, -3.4032e-01,\n",
       "          3.4861e-01,  3.5542e-01,  1.7158e-01,  1.0434e-02, -2.2489e-01,\n",
       "         -5.4942e-02,  6.1403e-01, -1.0975e-01,  1.7118e-01,  7.3632e-01,\n",
       "          2.5192e-01, -5.3250e-01, -1.2624e-01, -2.7634e-01,  1.3144e-01,\n",
       "          1.8390e-01, -3.1698e-01,  2.5134e-01,  4.8554e-01,  3.0017e-01,\n",
       "          7.3450e-01,  5.8561e-02,  1.0687e-01,  1.0761e-01,  2.4819e-01,\n",
       "         -1.0415e-01,  1.1396e-01,  6.4847e-02,  5.1507e-01,  4.5176e-01,\n",
       "         -3.9162e-01,  5.6699e-02, -2.1709e-01, -8.5214e-02, -1.7616e-01,\n",
       "         -4.7523e-01, -1.2134e-02,  3.3794e-01, -5.7909e-01,  6.4076e-02,\n",
       "         -2.1752e-01,  1.1341e-01, -8.7520e-02, -1.0765e-01, -2.3029e-01,\n",
       "         -4.0820e-01,  6.8080e-01, -3.9544e-02, -1.0757e-01, -4.4884e-01,\n",
       "         -4.0001e-01,  6.1110e-02,  2.0842e-01, -3.3845e-01, -2.4540e-01,\n",
       "          4.8759e-01, -6.4019e-02, -5.7059e-02, -1.6874e-01,  2.4369e-01,\n",
       "         -2.4259e-01,  2.5890e-01,  2.6719e-01, -2.1164e-01, -2.5709e-01,\n",
       "          7.0956e-02, -4.0929e-01, -3.3671e-01, -4.9956e-01,  5.5042e-01,\n",
       "         -3.4912e-01, -2.0008e-01, -2.3730e-01, -6.1538e-01,  1.3825e-01,\n",
       "          3.2920e-01,  5.2705e-01, -3.1624e-01,  1.0704e-01,  7.8889e-01,\n",
       "         -9.5615e-02, -3.5749e-01,  1.1383e-02,  3.4800e-01, -7.4367e-02,\n",
       "          6.4183e-01,  5.2562e-01, -2.2438e-02,  1.6308e-01,  7.2264e-01,\n",
       "          5.2338e-02,  1.9646e-01, -2.9163e-01,  5.3137e-01, -8.5046e-02,\n",
       "          2.9320e-01,  2.5419e-02,  5.0183e-02, -4.6959e-03, -1.2442e-01,\n",
       "          3.9640e-01,  4.8759e-01, -6.8770e-01, -2.0909e-01,  1.7391e-01,\n",
       "          4.1043e-02, -2.0719e-01, -4.2706e-01, -1.7395e-01, -3.4407e-01,\n",
       "         -1.2792e-01, -1.5234e-01, -7.6935e-02, -4.8482e-01, -1.4220e-01,\n",
       "          3.1179e-01, -4.3819e-01,  1.5216e-01,  6.0492e-01, -2.0609e-02,\n",
       "          3.0679e-01, -4.0934e-01, -8.8663e-02,  3.6965e-01, -1.8421e-01,\n",
       "         -3.6191e-01,  1.8609e-01,  8.4222e-01, -2.4798e-01, -7.3185e-01,\n",
       "         -6.7578e-02,  3.7685e-01,  6.6734e-02,  2.0544e-01, -1.0685e-01,\n",
       "         -3.0224e-01,  8.9721e-02, -9.9897e-04,  8.0792e-02, -2.6428e-01,\n",
       "         -5.8369e-01, -2.6540e-01,  5.9132e-01, -6.0704e-01,  8.4323e-02,\n",
       "         -1.7520e-01, -1.2734e-02, -4.6975e-01,  2.3337e-01, -3.4770e-01,\n",
       "          6.6131e-01,  1.6826e-01, -5.7643e-01,  1.0542e-01,  6.1783e-02,\n",
       "         -7.7056e-02, -1.7524e-01, -2.0857e-01,  7.0767e-01, -3.6453e-01,\n",
       "         -8.0199e-01,  1.8301e-01,  2.3899e-01,  6.3045e-01, -2.8039e-01,\n",
       "         -5.5762e-02, -2.3400e-01,  2.3956e-01,  1.8500e-01,  6.3159e-03,\n",
       "         -2.2380e-01, -1.7693e-01, -5.6956e-01, -3.7252e-01, -5.2656e-01,\n",
       "          1.0625e-01,  1.3280e-01, -5.8075e-01,  2.6341e-01, -1.4378e-01,\n",
       "          3.2977e-01,  2.0335e-01, -3.7786e-01, -1.0401e-01,  4.0183e-01,\n",
       "          4.7947e-01,  2.4861e-01,  4.9808e-01,  2.7402e-01,  2.3218e-01,\n",
       "         -2.4913e-01,  1.5188e-01,  1.7746e-01, -1.2191e-01,  4.9082e-01,\n",
       "         -1.4955e-01, -6.2619e-01, -1.0400e-01,  7.2152e-01,  9.9393e-02,\n",
       "         -3.5058e-01, -7.9178e-03,  5.9675e-01,  2.0104e-01,  7.0921e-02,\n",
       "          3.0117e-01, -4.0452e-01, -2.3563e-01, -7.2239e-02, -7.9153e-02,\n",
       "         -4.4002e-01, -2.4151e-01, -7.3683e-02, -1.5744e-01, -2.9276e-01,\n",
       "         -9.0456e-02, -2.2447e-01,  3.8692e-01, -5.8112e-01, -8.5422e-02,\n",
       "         -1.2147e-01, -9.2773e-02, -2.6203e-01,  1.5819e-01,  1.9313e-01,\n",
       "          3.7981e-02,  9.2126e-02,  6.3672e-01, -1.1875e-01, -1.6367e-01,\n",
       "         -2.0311e-01, -2.3932e-01,  2.7866e-01, -3.2459e-01,  9.4900e-02,\n",
       "         -1.0008e-01,  3.1609e-01, -5.9932e-01, -2.7006e-01,  7.5978e-04,\n",
       "         -2.6134e-01, -4.1266e-01,  4.4862e-01,  2.6931e-01,  2.1707e-01,\n",
       "          5.3670e-01,  9.6922e-02,  5.2163e-02, -3.3105e-01, -3.8879e-01,\n",
       "         -2.5432e-01,  2.3009e-01, -7.5707e-03, -4.7482e-01, -2.0281e-01,\n",
       "          2.3660e-01, -4.9760e-01, -2.5120e-01,  3.5086e-01, -9.4184e-04,\n",
       "         -1.8856e-01, -2.3253e-01, -1.5125e-01, -6.6113e-01,  1.6864e-01,\n",
       "          1.0934e-01,  7.1617e-02, -2.2990e-01,  1.0219e-01, -2.2715e-01,\n",
       "          1.8264e-01,  2.5516e-01,  9.1116e-02, -1.7003e-01, -4.3305e-01,\n",
       "         -4.9145e-01, -3.2139e-01,  5.7903e-02,  3.6657e-01, -2.2949e-01,\n",
       "         -2.8063e-01,  1.1617e-01,  3.6120e-01, -1.7741e-01,  1.1957e-01,\n",
       "          1.7351e-01, -6.6759e-01, -2.1306e-01, -4.7903e-02,  1.0976e-01,\n",
       "         -1.6574e-02, -2.5477e-01, -4.2668e-01,  3.1086e-01, -7.8112e-02,\n",
       "         -2.5410e-01,  5.9363e-01, -2.3090e-01,  4.0536e-01, -3.1344e-02,\n",
       "         -3.6027e-01, -1.7356e-01,  4.4256e-01, -2.4134e-02,  3.2344e-01,\n",
       "          1.2147e-01, -5.6970e-01, -6.8544e-02, -3.5827e-02, -2.9050e-01,\n",
       "         -2.6791e-01, -1.2770e-01, -1.2066e-01,  3.3045e-01, -6.2850e-01,\n",
       "          4.3790e-01,  8.3780e-02,  2.0555e-01,  2.2921e-01, -4.3751e-01,\n",
       "         -2.8730e-01,  2.6541e-01,  3.6385e-01, -2.7253e-01, -5.5142e-01,\n",
       "         -4.3237e-01, -3.5846e-01, -3.5458e-01, -2.6223e-01,  5.9106e-01,\n",
       "         -9.3076e-02, -2.7750e-01, -2.3211e-01,  5.5739e-01,  1.2421e-01,\n",
       "          1.8462e-03,  4.1556e-01,  1.6861e-01,  1.6488e-01,  2.2025e-01,\n",
       "         -7.3772e-01,  2.5997e-01, -3.9701e-01,  4.5173e-03, -8.4839e-02,\n",
       "          2.6684e-01, -3.0623e-01,  3.8680e-02, -3.4479e-01,  1.2279e-01,\n",
       "          3.9140e-01, -4.0717e-01, -1.4384e-01,  3.5290e-01,  2.1646e-01,\n",
       "         -3.4454e-01,  5.9429e-02,  1.9127e-01,  3.4930e-01,  1.3720e-01,\n",
       "          1.0680e-01,  4.4797e-01, -4.7895e-01,  3.2598e-03, -4.5932e-01,\n",
       "         -5.6631e-01,  2.2528e-01,  1.3527e-01,  3.9478e-01, -1.3899e-01,\n",
       "         -3.7747e-01,  6.4856e-01,  5.4442e-02, -7.5439e-02,  2.8162e-01,\n",
       "         -8.9302e-03, -5.4320e-02, -2.5094e-01,  1.9559e-01,  6.2133e-01,\n",
       "         -6.4973e-02,  9.0919e-02,  4.4535e-01, -5.6958e-01,  3.5233e-01,\n",
       "         -1.3435e-01, -1.8565e-02,  8.9066e-02]], device='cuda:0',\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(token_ids).to(device)[None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10318794-379b-437d-9627-f8cc875dc16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "sample_code = \"def max(a,b): if a>b: return a else return b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aa8cb9e-f189-4c10-a2fd-061a5b91e481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 9232, 19220, 1640, 102, 6, 428, 3256, 114, 10, 15698, 428, 35, 671, 10, 1493, 671, 741, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sample_code, padding=\"max_length\", truncation=True, max_length=256# # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f6e991-caf9-4044-921d-a2886518d6bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e786d42-887f-475c-ad55-2cb9935b9a89",
   "metadata": {},
   "source": [
    "# Fine-tuning v0\n",
    "## Create dataset\n",
    "This also creates the label2id and id2label maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c2125d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"Data Processing\", \"Web/API Code\", \"Algorithms/Logic\", \"Machine Learning\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a30fe07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695adec1d39f4c2f8f5148d6cf030c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5fa58a1e7444e8bad66f5426c4a2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df17e1c6cce4e979e1b043912dc10b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795503af17f242f8a02b42475dbd9c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"data/train.json\", \"test\": \"data/test.json\"})\n",
    "\n",
    "dataset = dataset.map(lambda x: {\"label\": label2id[x[\"label\"]]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc81e3-5b54-4091-a659-7300370edd6b",
   "metadata": {},
   "source": [
    "## Create tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a4e46def-4ace-4e7f-994c-2c71d441ab2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4134ff259fa04bdb802b8c30c0d55398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d313cae616c4ecfabb9f72518465616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"code\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d1cbe06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['code', 'label', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 36\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['code', 'label', 'input_ids', 'attention_mask'],\n",
       "         num_rows: 9\n",
       "     })\n",
       " }),\n",
       " RobertaTokenizerFast(name_or_path='microsoft/codebert-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       " \t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       " }\n",
       " ))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c62cbeb-84b0-4e19-bad2-96babc09a37c",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b98fb62c-22bf-43e0-ad38-e65e378f8230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/codebert-base\",\n",
    "    num_labels=4,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e2eff3-123a-4f1a-b7a5-25d1bd89be67",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1030af7c-11e6-4494-b84c-48a504152d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./codebert-finetuned\",\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_on_start=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3bdf5543-4cb7-4a1b-afb4-4f04dd79cf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_580285/1908840112.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5fcfea27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1917643547058105,\n",
       " 'eval_model_preparation_time': 0.0036,\n",
       " 'eval_runtime': 0.1914,\n",
       " 'eval_samples_per_second': 47.03,\n",
       " 'eval_steps_per_second': 5.226}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c34ec-8998-4b47-babd-6e2ec3c3a977",
   "metadata": {},
   "source": [
    "## Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7dff51ae-4ceb-4377-9d4a-dd697d855d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_code(code_snippet):\n",
    "    inputs = tokenizer(code_snippet, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    outputs = model(**inputs.to(device))\n",
    "    probs = outputs.logits.softmax(dim=1)\n",
    "    pred = probs.argmax(dim=1).item()\n",
    "    return id2label[pred], probs[0][pred].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8d55569-b7e5-4bf0-8e7d-853de701964f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Data Processing', 0.335357129573822)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_code = \"\"\"\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\"\"\"\n",
    "classify_code(sample_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf35a5d-f5cc-4c61-a1ed-cec65aa40682",
   "metadata": {},
   "source": [
    "Estaría bueno que el scraper acumule en lugar de borrar. Que se guarde el hash para no duplicar cosas y que haya un sistema de reviews. Pienso que estaría genial que muestre el código formateado, que haya un botón para aprobar y pasar al siguiente o reprobar y hacer un soft delete antes de avanazar. Tiene que ser un soft delete para que no se vuelvan a agregar y revisar códigos ya revisados. \n",
    "\n",
    "Quizá incluso estaría bueno que no se procese dos veces la misma página. Que no se guarde sólo un pedazo de código (o su hash), sino una URI. Y que no se tomen dos pedazos de código del mismo lugar. Así, la diversidad será máxima.\n",
    "\n",
    "Pero todo esto puede quedar para una segunda revisión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb9d70f",
   "metadata": {},
   "source": [
    "# Migrar a pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3c6fb34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (2025.7.34)\n",
      "Requirement already satisfied: requests in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (2.32.5)\n",
      "Requirement already satisfied: hydra-core in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: omegaconf in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: bitarray in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (3.7.1)\n",
      "Requirement already satisfied: tqdm in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (4.67.1)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.40.21-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from hydra-core) (4.9.3)\n",
      "Requirement already satisfied: packaging in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from hydra-core) (25.0)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from omegaconf) (6.0.2)\n",
      "Collecting botocore<1.41.0,>=1.40.21 (from boto3)\n",
      "  Downloading botocore-1.40.21-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3)\n",
      "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from botocore<1.41.0,>=1.40.21->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.21->boto3) (1.17.0)\n",
      "Requirement already satisfied: click in /home/mggp/localwork/pythula/.venv/lib/python3.10/site-packages (from sacremoses) (8.2.1)\n",
      "Collecting joblib (from sacremoses)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Downloading boto3-1.40.21-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.40.21-py3-none-any.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
      "Downloading sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Installing collected packages: sentencepiece, joblib, jmespath, sacremoses, botocore, s3transfer, boto3\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [boto3]32m5/7\u001b[0m [s3transfer]\n",
      "\u001b[1A\u001b[2KSuccessfully installed boto3-1.40.21 botocore-1.40.21 jmespath-1.0.1 joblib-1.5.2 s3transfer-0.13.1 sacremoses-0.1.1 sentencepiece-0.2.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install regex requests hydra-core omegaconf bitarray tqdm boto3 requests regex sentencepiece sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a2bc06f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/main\" to /home/mggp/.cache/torch/hub/main.zip\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "operator torchvision::nms does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpytorch/vision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/localwork/pythula/.venv/lib/python3.10/site-packages/torch/hub.py:482\u001b[0m, in \u001b[0;36mlist\u001b[0;34m(github, force_reload, skip_validation, trust_repo, verbose)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _add_to_sys_path(repo_dir):\n\u001b[1;32m    481\u001b[0m     hubconf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(repo_dir, MODULE_HUBCONF)\n\u001b[0;32m--> 482\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m \u001b[43m_import_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODULE_HUBCONF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhubconf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# We take functions starts with '_' as internal helper functions\u001b[39;00m\n\u001b[1;32m    485\u001b[0m entrypoints \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    486\u001b[0m     f\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(hub_module)\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(hub_module, f)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    489\u001b[0m ]\n",
      "File \u001b[0;32m~/localwork/pythula/.venv/lib/python3.10/site-packages/torch/hub.py:115\u001b[0m, in \u001b[0;36m_import_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m    113\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mmodule_from_spec(spec)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, Loader)\n\u001b[0;32m--> 115\u001b[0m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/torch/hub/pytorch_vision_main/hubconf.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Optional list of dependencies required by the package\u001b[39;00m\n\u001b[1;32m      2\u001b[0m dependencies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_model_weights, get_weight\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malexnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m alexnet\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convnext_base, convnext_large, convnext_small, convnext_tiny\n",
      "File \u001b[0;32m~/.cache/torch/hub/pytorch_vision_main/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/torch/hub/pytorch_vision_main/torchvision/_meta_registrations.py:164\u001b[0m\n\u001b[1;32m    153\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[1;32m    154\u001b[0m         grad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m rois\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m         ),\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mnew_empty((batch_size, channels, height, width))\n\u001b[1;32m    163\u001b[0m \u001b[38;5;129;43m@torch\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchvision::nms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mmeta_nms\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes should be a 2d tensor, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes should have 4 elements in dimension 1, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/localwork/pythula/.venv/lib/python3.10/site-packages/torch/library.py:1069\u001b[0m, in \u001b[0;36mregister_fake.<locals>.register\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1068\u001b[0m     use_lib \u001b[38;5;241m=\u001b[39m lib\n\u001b[0;32m-> 1069\u001b[0m \u001b[43muse_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_fake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacklevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_override\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_override\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
      "File \u001b[0;32m~/localwork/pythula/.venv/lib/python3.10/site-packages/torch/library.py:219\u001b[0m, in \u001b[0;36mLibrary._register_fake\u001b[0;34m(self, op_name, fn, _stacklevel, allow_override)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     func_to_register \u001b[38;5;241m=\u001b[39m fn\n\u001b[0;32m--> 219\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfake_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_to_register\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_override\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_override\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registration_handles\u001b[38;5;241m.\u001b[39mappend(handle)\n",
      "File \u001b[0;32m~/localwork/pythula/.venv/lib/python3.10/site-packages/torch/_library/fake_impl.py:50\u001b[0m, in \u001b[0;36mFakeImplHolder.register\u001b[0;34m(self, func, source, lib, allow_override)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an fake impl registered at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m     )\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_has_kernel_for_dispatch_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqualname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an DispatchKey::Meta implementation via a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_dispatch_has_kernel_for_dispatch_key(\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompositeImplicitAutograd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m ):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.hub.list(\"pytorch/vision\", force_reload=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
